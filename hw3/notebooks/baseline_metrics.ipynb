{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from minisearch.search import Searcher \n",
    "from minisearch.index import PositionalIndex, tokenize\n",
    "\n",
    "from minisearch.fast_ranking import FastRanker\n",
    "from minisearch.quorum import QuorumCandidateGenerator, QuorumConfig\n",
    "\n",
    "import aux\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "aux.download_and_sample(force_reload=False, n_queries=2000, n_docs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101937/101937 [01:25<00:00, 1187.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# build index\n",
    "docs_df, queries_df, qrels = aux.load_data()\n",
    "index, int_to_str_id = aux.build_index(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ranker\n",
    "weights = [0, 1.0, 0, 1.0, 0.6]\n",
    "ranker = FastRanker(index, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**big quorum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build quorum\n",
    "cfg = QuorumConfig(\n",
    "    activate_if_candidates_lt=2000,\n",
    "    target=5000,\n",
    "    cap=50000,\n",
    "    anchor_pool=8,\n",
    "    max_df_frac=0.2,\n",
    "    k_frac=0.35,\n",
    "    min_k=2,\n",
    "    max_universe=200000\n",
    "    )\n",
    "\n",
    "quorum = QuorumCandidateGenerator(index, tokenize_fn=tokenize, config=cfg)   \n",
    "searcher = Searcher(index, ranker, quorum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2000/2000 [03:05<00:00, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5115\n",
      "NDCG@1: 0.5115\n",
      "MRR@10: 0.6052\n",
      "NDCG@10: 0.6487\n",
      "MRR@100: 0.6094\n",
      "NDCG@100: 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = aux.eval_ranking(searcher, queries_df, qrels, int_to_str_id, ks=(1, 10, 100))\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small quorum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build quorum\n",
    "cfg = QuorumConfig(activate_if_candidates_lt=1000, target=1000, cap=5000)\n",
    "\n",
    "quorum = QuorumCandidateGenerator(index, tokenize_fn=tokenize, config=cfg)   \n",
    "searcher = Searcher(index, ranker, quorum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2000/2000 [01:36<00:00, 20.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5115\n",
      "NDCG@1: 0.5115\n",
      "MRR@10: 0.6048\n",
      "NDCG@10: 0.6479\n",
      "MRR@100: 0.6089\n",
      "NDCG@100: 0.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = aux.eval_ranking(searcher, queries_df, qrels, int_to_str_id, ks=(1, 10, 100))\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dummy quorum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build quorum\n",
    "cfg = QuorumConfig(activate_if_candidates_lt=1000, target=50, cap=2000)\n",
    "\n",
    "quorum = QuorumCandidateGenerator(index, tokenize_fn=tokenize, config=cfg)   \n",
    "searcher = Searcher(index, ranker, quorum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2000/2000 [01:07<00:00, 29.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.4920\n",
      "NDCG@1: 0.4920\n",
      "MRR@10: 0.5641\n",
      "NDCG@10: 0.5939\n",
      "MRR@100: 0.5649\n",
      "NDCG@100: 0.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = aux.eval_ranking(searcher, queries_df, qrels, int_to_str_id, ks=(1, 10, 100))\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:22<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6055\n",
      "MRR@10: 0.6759392857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "ce = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "doc_text = aux.make_doc_text_map(docs_df, id_col=\"doc_id\", text_col=\"body\")\n",
    "\n",
    "mrr1 = []\n",
    "mrr10 = []\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import metrics\n",
    "\n",
    "for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "    qid = int(row[\"query_id\"])\n",
    "    if qid not in qrels:\n",
    "        continue\n",
    "    text = row[\"text\"]\n",
    "    targets = qrels[qid]\n",
    "\n",
    "    results_int = [doc_id for doc_id, _ in searcher.search(text, top_k=100)]\n",
    "    results_int = aux.rerank_with_cross_encoder(searcher, text, results_int, int_to_str_id, doc_text, ce, rerank_k=10)\n",
    "\n",
    "    results_str = [int_to_str_id[i] for i in results_int]\n",
    "    mrr1.append(metrics.mrr(results_str, targets, k=1))\n",
    "    mrr10.append(metrics.mrr(results_str, targets, k=10))\n",
    "\n",
    "print(\"MRR@1:\", float(np.mean(mrr1)))\n",
    "print(\"MRR@10:\", float(np.mean(mrr10)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
